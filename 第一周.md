# 1. 引言（Introduction to Machine Learning）

## 1.1 什么是机器学习？

第一个提出机器学习的是Arthur Samuel，她将机器学习定义为一种研究领域，使计算机能够在没有明确编程的情况下进行学习。在20世纪50年代，她写了一个跳棋游戏程序，使得电脑在跟电脑自己下棋的过程中进行自我训练，最终成为一个好的棋手。

机器学习的两种主要类型是监督学习（supervised learning）和非监督学习（unsupervised learning）。到目前为止，最常用的学习算法类型是监督学习、无监督学习和推荐系统（recommender systems）。接下来会对这些定义进行具体介绍。

# 2. 监督学习和无监督学习（Supervised Vs. Unsupervised Machine Learning）


## 2.1 监督学习（supervised learning）

监督学习是指学习x到y或输入到输出映射的算法。

监督学习的关键特征是你需要提供学习算法例子来学习。通过使用带标签的数据集训练算法，以达到准确分类数据或预测结果的目的。

><small>今天最有利润的监督学习形式可能是用于在线广告。几乎所有的大型在线广告平台都有一个学习算法，它会输入关于广告的一些信息和关于你的一些信息，然后试图弄清楚你是否会点击那个广告。因为通过向你展示广告，他们只是更有可能点击，对于这些大型在线广告平台来说，每一次点击都是收入，这实际上为这些公司带来了很多收入。这是我曾经做过很多工作的东西，也许不是最鼓舞人心的应用，但它确实对今天一些国家的经济产生了重大影响。<br> <br>或者，如果你想制造一辆自动驾驶汽车，学习算法会把一张图像和其他传感器(如雷达或其他东西)的一些信息作为输入，然后尝试输出其他汽车的位置，比如，其他汽车的位置，这样你的自动驾驶汽车就可以安全地绕过其他汽车。<br> <br>或者以制造业为例，你可以让一个学习算法把一个制造出来的产品的图片作为输入，比如一个刚刚从生产线上下线的手机，然后让学习算法输出产品中是否有划痕、凹痕或其他缺陷。这被称为视觉检查，它可以帮助制造商减少或防止产品缺陷。</small>


在所有这些应用中，你将首先用输入x和正确答案的例子来训练你的模型，也就是标签y。在模型从这些输入、输出或(x, y)中学习之后，他们可以使用一个全新的输入x，它从未见过的东西，并尝试产生相应的输出y。让我们更深入地研究一个具体的例子。

### 2.1.1 回归算法


![[Pasted image 20230131165804.png]]
假设你想根据房子的大小来预测房价。你收集了一些数据，然后把数据画出来，它是这样的。横轴上是房子的面积，单位是平方英尺。纵轴是房子的价格，以几千美元为单位。有了这些数据，假设一个朋友想知道他们750平方英尺的房子的价格。学习算法如何帮助你?学习算法可能会做的一件事是，对于数据的直线，从直线上读取，看起来你朋友的房子可以卖到15万美元。但是拟合直线并不是你唯一可以使用的学习算法。对于这个应用程序，还有其他更好的方法。

![[Pasted image 20230131170014.png]]

现在看起来拟合曲线似乎是更好的选择，那么看起来，你朋友的房子可以卖到接近20万美元。

在这张幻灯片上显示的是监督学习的一个例子。因为我们给了算法一个数据集，在这个数据集里，所谓的正确答案，也就是标签，或者说正确的价格y，是给图上每一栋房子的。学习算法的任务是产生更多这样的正确答案，特别是预测其他房子的可能价格，比如你朋友的房子。

这种房价预测是一种特殊类型的监督学习，称为回归。通过回归，我们试图从无限个可能的数字中预测一个数字，这就是监督学习，学习输入，输出，或者x到y的映射。你在这个视频中看到了一个回归的例子，回归这个词的意思是，**我们在试着推测出这一系列连续值属性**。但是还有第二种主要的监督学习问题叫做分类。

### 2.1.2 分类算法

以乳腺癌检测作为例子。假设你正在构建一个机器学习系统，这样医生就可以有一个诊断工具来检测乳腺癌。利用病人的医疗记录，你的机器学习系统试图找出一个肿块是恶性的，意味着癌变或危险。在这个例子中用0表示良性的，用1表示恶性的。

![[Pasted image 20230131171022.png]]

然后可以将数据绘制在这样的图形上，其中横轴表示肿瘤的大小，垂直轴只有两个值(0或1)，这取决于肿瘤是良性0还是恶性1。这与回归不同的一个原因是，**我们试图只预测少量可能的输出或类别**。

在这种情况下，有两个可能的输出0或1，良性或恶性。这不同于回归，回归试图预测任何数字，所有无限个可能的数字。因此，只有两种可能的输出就是这种分类的原因。

当你解释数字时，分类与回归的不同之处在于，分类预测的是一个小的、有限的、可能的输出类别集，如0、1和2，而不是介于0.5或1.7之间的所有可能的数字。

![[Pasted image 20230131171318.png]]

也可以使用多个输入值来预测输出。举个例子，除了知道肿瘤的大小，你还知道每个病人的年龄。您的新数据集现在有两个输入，年龄和肿瘤大小。在这个新的数据集中，我们将用圆圈来表示良性肿瘤的患者，用十字来表示恶性肿瘤的患者。因此，当有新病人来看病时，医生可以测量病人的肿瘤大小，并记录病人的年龄。鉴于此，我们如何预测这个病人的肿瘤是良性还是恶性。假设有这样的一天，学习算法可能会找到一些边界，将恶性肿瘤和良性肿瘤分开。所以学习算法必须决定如何在这些数据中拟合出一条**边界线**。学习算法发现的边界线将帮助医生进行诊断。在这种情况下，肿瘤更可能是良性的。

> <small>你想用无限多种特征，好让你的算法可以利用大量的特征，或者说线索来做推测。那你怎么处理无限多个特征，甚至怎么存储这些特征都存在问题，你电脑的内存肯定不够用。我们以后会讲一个算法，叫<b>支持向量机</b>，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。</small>

## 2.2 无监督学习（unsupervised learning）

![[Pasted image 20230131175401.png]]

在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子， 即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据 集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。 这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同 的簇。所以叫做聚类算法。

聚类应用的一个例子就是在谷歌新闻中。谷歌新闻每天都在收集非常多的网络的新闻内容。 它再将这些新闻分组，组成有关联的新闻。所以谷歌新闻做的就是搜索非常多的新闻事件， 自动地把它们聚类到一起。所以，这些新闻事件全是同一主题的，所以显示到一起。

![[Pasted image 20230131175624.png]]

其中就有基因学的理解应用。一个 DNA 微观数据的例子。基本思想是输入一组不同个 体，对其中的每个个体，你要分析出它们是否有一个特定的基因。技术上，你要分析多少特 定基因已经表达。所以这些颜色，红，绿，灰等等颜色，这些颜色展示了相应的程度，即不 同的个体是否有着一个特定的基因。你能做的就是运行一个聚类算法，把个体聚类到不同的 类或不同类型的组（人）。

**这是一种无监督学习算法，它获取没有标签的数据，并试图自动将它们分组到集群中**。除了聚类，还有其他类型的无监督学习。

# 3. 回归模型（regression model）

## 3.1 线性回归模型（Linear regression model）

让我们从一个可以使用线性回归解决的问题开始。假设你想根据房子的大小来预测房子的价格。我们将使用来自美国波特兰的房屋大小和价格数据集。这里我们有一个图表，横轴是房子的大小，单位是平方英尺，纵轴是房子的价格，单位是几千美元。让我们继续绘制数据集中不同房屋的数据点。这里的每一个数据点，每一个小叉都是一个房子的大小和最近的售价。
![[Pasted image 20230131182925.png]]

现在，假设你是波特兰的一名房地产经纪人，你正在帮助一位客户出售她的房子。她是在问你，你觉得这房子我能卖多少钱？这个数据集也许能帮你估计一下她能卖多少钱。你从测量房子的大小开始，结果是房子有1250平方英尺。你觉得这房子能卖多少钱？你可以做的一件事是，你可以从这个数据集建立一个线性回归模型。您的模型将拟合数据的直线，它可能看起来像这样。根据这条与数据匹配的直线，你可以看到房子是1250平方英尺，它将与这里的最佳拟合线相交，如果你追踪到左边的纵轴，你可以看到价格可能在这里，大约22万美元。

这就是所谓的监督学习模型的一个例子。我们称之为监督学习，因为你首先是通过给出一个有正确答案的数据来训练一个模型，因为你得到了房子的模型示例，包括房子的大小，以及模型应该预测的每套房子的价格。这里是价格，也就是说，数据集中每个房子的正确答案都给出了。

这种线性回归模型是一种特殊类型的监督学习模型。它被称为回归模型，因为**它预测数字作为产出**，如美元价格。

![[Pasted image 20230131183029.png]]

在回归中，模型可以输出无限多个可能的数字。除了将这些数据可视化为左边的图表，还有一种方法可以很有用，那就是右边的数据表。数据由一组输入组成。这就是房子的大小，也就是这一列。它也有输出。你要预测价格，也就是这一列。请注意，水平轴和垂直轴对应这两列，大小和价格。假设这个数据表中有47行，那么左边的图上就有47个小叉，每个叉对应表中的一行。

**专业术语：**

![[Pasted image 20230131184220.png]]

- **训练集**：用来**训练模型的数据集**叫做**训练集**。您客户的房子不在这个数据集中，因为它还没有出售，所以没有人知道价格是多少。为了预测你客户的房子的价格，你首先训练你的模型从训练集中学习，然后这个模型就可以预测你客户的房子的价格。
- ${x - }$**输入变量/特征**：在机器学习中，这里**表示输入的标准符号是小写的x**，我们称其为**输入变量**，也称为**特征或输入特征**。例如，对于训练集中的第一个房子，x是房子的大小，所以x等于2104。
-  ${y - }$**输出变量/目标变量**：用来表示你要预测的输出变量的标准符号，有时也被称为**目标变量**，是小写的y，这里，y是房子的价格，在第一个训练例子中，它等于400，所以y等于400。
-  ${m - }$**训练样本数量**：数据集每所房子都有一行，在这个训练集中，有47行，每行代表一个不同的**训练示例**。我们用小写的**m**表示**训练样本的总数**，这里m等于47。
- ${(x, y) - }$**单个训练示例**：为了表示单个训练示例，我们将使用括号x, y。对于第一个训练示例(x, y)，这对数字是(2104,400)。
- ${(x^{(i)}, y^{(i)}) - }$**特定训练示例**：第i个训练示例，这将对应于左侧表格中的特定行，我将在括号中使用符号x上标，在括号I中使用符号I, y上标。上标告诉我们这是第I个训练示例。

![[Pasted image 20230131191805.png]]

回想一下，监督学习中的训练集既包括输入特征，比如房子的大小，也包括输出目标，比如房子的价格。输出目标是我们将从中学习的模型的正确答案。为了训练模型，你把训练集、输入特征和输出目标都输入到学习算法中。然后你的监督学习算法会产生一些函数。我们把这个函数写成小写的f，有时也表示为h。

f的作用是得到一个新的输入x和输出，然后估计或预测，它叫做$\hat{y}$。在机器学习中，惯例是$\hat{y}$是y的估计值或预测值。函数f称为模型。X被称为输入或输入特征，模型的输出是预测，$\hat{y}$。

模型的预测是y的估计值。当符号只是字母y时，则它指的是目标，即训练集中的实际真实值。相反，$\hat{y}$是估计值。它可能是也可能不是实际的真实值。如果你在帮助你的客户卖房子，房子的真实价格在他们卖之前是不知道的。

现在，当我们设计一个学习算法时，一个关键问题是，我们如何表示函数f？换句话说，我们要用什么数学公式来计算f?现在，我们坚持f是一条直线。

![[Pasted image 20230131192342.png]]

你的函数可以写成$f_{w,b}(x) = wx + b$。但是现在，只需要知道w和b是数字，为w和b选择的值将决定基于输入特征x的预测，$\hat{y}$。这个$f_{w,b}(x)$意味着f是一个以x为输入的函数，根据w和b的值，f将输出预测$\hat{y}$的某个值。作为另一种写法，有时只写f(x)。

让我们把训练集画在图上，输入特征x在横轴上，输出目标y在纵轴上。

记住，算法从这些数据中学习并生成最拟合的直线，比如这条。这个线性函数$f_{w,b}(x) = wx + b$。我们可以降低w和b和只写$f(x) = wx + b$。这是这个函数是要预测y的值,使用一个简化的函数x。

有时候你也想拟合更复杂的非线性函数，比如这样的曲线。但由于这个线性函数相对简单，易于处理，让我们使用一条线作为基础，最终将帮助你得到更复杂的非线性模型。这个模型有个名字，叫做线性回归。更具体地说，这是**单变量线性回归**。

![[Pasted image 20230131194105.png]]

为了实现线性回归，第一个关键步骤是首先定义一个成本函数。成本函数会告诉我们模型的表现如何，这样我们就能让它做得更好。你有一个包含输入特征x和输出目标y的训练集。你要用来适应这个训练集的模型是这个线性函数$f_{w,b}(x) = wx + b$。w和b被称为模型的参数。在机器学习中，模型的参数是你在训练过程中可以调整的变量，以改进模型。有时你也会听到参数w和b被称为系数或权重。根据你选择的w和b的值你会得到一个不同的函数f(x)它会在图上生成一条不同的直线。

![[Pasted image 20230131194427.png]]

第一个例子，函数f (x)等于0 * (x + 1.5)所以f总是一个常数。y的估计值总是1.5，$\hat{y}$总是等于b这里b也被称为y截距。
第二个例子，如果w = 0.5且b = 0，则f (x) = 0.5 * x。当x = 0时，预测结果也是0，当x = 2时，预测结果是0.5 * 2，即1。得到一条这样的直线，斜率是0.5。w的值表示直线的斜率，也就是0.5。
最后，如果w = 0.5，b = 1，那么f (x) = 0.5 (x + 1)。当x = 0时，f (x) = b，也就是1。所以这条直线与纵轴在b处相交，即y轴截距。当x = 2时，f (x) = 2，直线是这样的。同样，这个斜率是0.5除以1所以w的值给你的斜率是0.5。

回想一下，你有一个类似于这里显示的训练集。在线性回归中，你要做的是为参数w和b选择值，这样你从函数f中得到的直线就能很好地符合数据。比如这条线。


比如这个点是由${(x^{(i)}, y^{(i)})}$定义的，其中y是目标。对于给定的输入$x^i$，函数f也给出了y的预测值，它对y的预测值就是这里的$y^i$。对于我们选择的模型$f_{w,b}(x^{(i)}) = wx^{(i)} + b$，换一种说法，预测$\hat{y}^{(i)} = f_(w,b)(x^{(i)})$。


现在的问题是如何找到w和b的值使得预测$y^i$接近真实的目标$\hat{y}^{i}$。要回答这个问题，让我们先来看看如何衡量一条线与训练数据的拟合程度。为此，我们要构造一个成本函数。

![[Pasted image 20230131200750.png]]

1. 成本函数使用预测$\hat{y}$，并将其与目标y进行比较，即$\hat{y} - y$。这种差被称为误差，我们测量的是预测距离目标的距离。
2. 接下来，让我们计算这个误差的平方。同时，我们还想计算训练集中不同训练例子i的这一项。当测量误差时，例如i，我们会计算这个平方误差项。
3. 最后，我们想要测量整个训练集的误差。特别地，让我们像这样把平方误差加起来。我们从i = 1 2 3一直加到m，m是训练样本的数量，这个数据集是47。
4. 注意，如果我们有更多的训练示例，m就会更大，成本函数就会计算出更大的数字。
5. 为了建立一个不会随着训练集规模变大而自动变大的代价函数，我们将计算**平均平方误差**，而不是总平方误差，我们通过像这样**除以m**来计算。
6. 按照惯例，机器学习人员使用的代价函数实际上是除以2乘以m。额外的除以2只是为了让我们后面的一些计算看起来更整洁，但无论你是否包括这个除以2，代价函数仍然有效。

这也叫**平方误差代价函数**，之所以叫这个是因为要取这些误差项的平方。

## 3.2 线性回归模型的可视化

![[Pasted image 20230201194221.png|450]]

回顾一下，这是目前为止我们所看到的成本函数。你想要为训练数据拟合一条直线，所以你有这个模型。这里，模型的参数是w和b。现在，根据这些参数选择的值，你得到了不同的直线，像这样。你想要找到w和b的值，这样直线就能很好地匹配训练数据。

为了衡量w和b的选择是否适合训练数据，你有一个成本函数J，成本函数J所做的是，它测量模型的预测和y的真实值之间的差异。你稍后会看到，线性回归会试图找到w和b的值，然后让J(w)尽可能小。

![[Pasted image 20230201194705.png|425]]

现在，为了让我们更好地可视化代价函数J，这是线性回归模型的简化版本。你可以把它想象成把参数b设为0。你现在只有一个参数w，而代价函数J看起来和以前差不多。

下图为$f_w (x)$和$J(w)$的图像：

![[Pasted image 20230201195208.png]]

w = 0.5时图像为：

![[Pasted image 20230201195411.png]]

再去修改w的值：

![[Pasted image 20230201195527.png]]

J是成本函数，用来衡量平方误差的大小，所以选择w使平方误差最小化，使它们尽可能小，会给我们一个好的模型。

在这个例子中，如果你要选择w的值使J(w)的值最小你最终会选择w = 1。

这使得直线很好地拟合了训练数据。这就是在线性回归中如何使用代价函数来找到使J最小的w的值。在更一般的情况下，我们有参数w和b，而不是只有w，你可以找到使J最小的w和b的值。

**线性回归的目标是找到参数w或w和b，使代价函数J的值可能最小。**

事实证明，成本函数有类似汤碗的形状：

![[Pasted image 20230201200232.png]]

你在这里看到的是一个三维曲面图，其中轴被标记为w和b。成可以看出在三维空间中存在一个使得𝐽(w,b)最小的点。有时将其图形化可以帮助我们解决问题。

![[Pasted image 20230201201114.png]]

让我们看看更多w和b的可视化。这里有一个例子。在这里，在图j上有一个特殊的点，对于这个点，w=- 0.15，b=800。这个点对应于w和b的一对值，它们使用了特定的cost j。事实上，w和b的这一对值对应于这个函数f(x)，就是左边这条线。这条线与纵轴相交于800点，因为b = 800，这条线的斜率是- 0.15，因为w = - 0.15。现在，如果你观察训练集中的数据点，你可能会注意到这条线与数据不太吻合。

对于这个函数f (x)，有了这些w和b的值，许多对y值的预测与训练数据中y的实际目标值相差甚远。因为这条线不是很适合，如果你看j的图形，这条线的cost在这里，离最小值还很远。代价相当高因为w和b的选择并不适合训练集。

![[Pasted image 20230201201315.png]]

如果你看左边的f(x)它看起来很适合训练集。你可以看到在右边，这个代表成本的点非常接近小椭圆的中心，它不是最小值，但已经很接近了。对于这个w和b的值，你得到这条线——f (x)。如果你在直线上测量数据点和预测值之间的垂直距离，你会得到每个数据点的误差。所有这些数据点的误差平方和非常接近于所有可能的直线拟合中误差平方和的最小可能和。

现在在线性回归中，手动尝试读取等高线图以获得w和b的最佳值不是一个很好的过程。你真正想要的是一个有效的算法，你可以用代码来自动找到参数w和b的值，它们会给你最好的拟合直线。这使得代价函数J最小化，有一种算法叫做梯度下降。该算法是机器学习中最重要的算法之一。梯度下降和梯度下降的变体被用于训练，不仅是线性回归，还有所有人工智能中一些最大、最复杂的模型。下章介绍梯度下降。

# 4. 用梯度下降法训练模型（Train the model with gradient descent）

## 4.1 梯度下降

梯度下降在机器学习中被广泛使用，不仅用于线性回归，还用于例如一些最先进的神经网络模型(也称为深度学习模型)的训练。


你有代价函数$J (w, b)$，你想要最小化它。在我们目前看到的例子中，这是线性回归的成本函数，但结果证明梯度下降是一种算法，你可以用它来最小化任何函数，而不仅仅是线性回归的成本函数。

例如，如果你有一个成本函数J作为$w_1$到$w_n$和b的函数，你的目标是最小化参数$w_1$到$w_n$和b上的J。换句话说，你想为$w_1$到$w_n$和b选择值，这让你得到J的最小值。事实证明，梯度下降是一种算法，你可以应用来尝试最小化函数J。你要做的只是从w和b的一些初始猜测开始。在线性回归中，**初始值是什么并不重要，所以一个常见的选择是将它们都设为0**。

![[Pasted image 20230201202629.png]]

用梯度下降算法，你要做的就是，不断地改变参数w和b，每次都试图减少J直到达到或接近一个最小值。要注意的一点是，对于一些函数可能存在多个可能的最小值。

让我们看一个更复杂的例子，看看梯度是做什么的。这个函数不是一个**平方误差代价函数**。下面的轴是w和b,对于不同的w和b值，你会得到这个曲面上不同的点$J (w, b)$。其中曲面在某一点的高度就是成本函数的值。

现在，让我们想象这个表面图实际上是一个稍微多山的户外公园或高尔夫球场的视图，高点是山，低点是谷，就像这样。你们正站在山上的这一点上，目标是从这里开始尽可能高效地到达这些山谷的底部。

梯度下降算法所做的是，你会旋转360度，环顾四周，然后问自己，如果我朝一个方向迈出一小步，我想尽快下山到这些山谷中的一个。迈出这一小步，**我要选择什么方向**？如果你想尽可能高效地走下山，如果你站在山上的这个点上环顾四周，你会注意到下一步下坡的最佳方向大概就是这个方向。从数学上讲，**这是下降速度最快的方向**。这意味着当你迈出一小步时，它会比你在其他方向迈出的一小步更快地让你下坡。迈出第一步之后，我们重复这个过程，直到你发现自己在这个山谷的底部，在这个局部极小值。

你刚刚做的是通过梯度下降的多个步骤。事实证明，梯度下降有一个有趣的性质。你可以通过为参数w和b选择起始值来选择曲面上的起点。如果你第二次使用梯度下降，从第一次的右边开始，你会得到一个完全不同的山谷。右边这个不同的最小值。第一个和第二个山谷的底部被称为**局部极小值**。

![[Pasted image 20230201203219.png]]

### 4.1.1 实现方法

![[Pasted image 20230202201949.png]]

$\alpha$ 是学习率，通常是0到1之间的一个数。它的作用是，它基本上控制了你下坡的幅度。
最后这一项，是成本函数J的导数项。它告诉你你想从哪个方向迈出第一步。结合学习率$\alpha$ ，它还决定了您想要下山的步骤的大小。

![[Pasted image 20230202202255.png]]

你的模型有两个参数，不仅是w，还有b，还有一个赋值操作，更新参数b。

在曲面图的图中，你只是一步一步地走到值的底部，对于梯度下降算法，你要重复这两个更新步骤，直到算法收敛。所谓收敛，是你达到了一个局部最小值的点在这个点上，参数w和b不再随着每一步的增加而发生很大的变化。

你将更新两个参数，w和b。这个更新发生在两个参数w和b中。一个重要的细节是，对于梯度下降，你想同时更新w和b，这意味着你想同时更新两个参数。

![[Pasted image 20230202202651.png]]

这是实现梯度下降的正确方法，同时进行更新。这里设置了一个变量temp_w和一个变量temp_b，两边都更新，并将它们存储到变量temp_w和temp_b中。然后将temp_w的值复制到w中，将temp_b的值复制到b中。

这就是梯度下降。

**小结：**

- 学习率控制下坡幅度
- 有两个参数：w，b，同步更新，直至收敛

### 4.1.2 学习率

$\alpha$ 会对你实现梯度下降的效率产生巨大的影响。如果学习率选择得不好下降率可能根本不起作用。
如果学习率太小，虽然梯度还是在下降，但会很慢；如果学习率太大，那么梯度下降可能会过度，可能永远不会达到最小值。另一种说法是，大的相交点可能不会收敛，甚至可能发散。


还有一个问题，如果我们预先把w放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？

假设你将w初始化在局部最低点， 结果是局部最优点的导数将等于零，因为它是那条切线的斜率。它使得w不再改变，也就是新的w等于原来的w，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使 学习速率𝑎保持不变时，梯度下降也可以收敛到局部最低点。

为了说明这一点，让我们看另一个例子：

![[Pasted image 20230202205054.png|350]]

这是我们想要最小化的代价函数J(w)，我们在粉红色的圈处初始化梯度下降。如果我们采取一个更新步骤，也许它会把我们带到最低点。现在，我们到第二个点上，我们要再走一步。你可能会注意到斜率不像第一点那么陡了。所以导数没有那么大。所以下一个更新步骤不会像第一步那么大。到了第三个点，导数比上一步小。当我们接近最小值时，它会走更小的一步，导数越来越接近于零。

所以当我们进行梯度下降时，我们最终会采取非常小的步骤，直到最终达到局部最小值。回顾一下，当我们接近局部最小梯度下降时，会自动采取更小的步骤。这是因为当我们接近局部极小值时，导数会自动变小。这意味着更新步骤也会自动变小。即使学习率$\alpha$保持在某个固定值。

## 4.2 线性回归中的梯度下降

![[Pasted image 20230202205539.png]]

左边是线性回归模型，右边是平方误差成本函数，下面是梯度下降算法。
你可以用这些公式来计算这两个导数，并以这种方式实现梯度下降。

但是，应该从哪里得到这些公式？

![[Pasted image 20230202205823.png]]

经过微积分计算，现在你有了这两个导数的表达式，你可以把它们代入梯度下降算法。

![[Pasted image 20230202205941.png]]

这是线性回归的梯度下降算法，你重复地对w和b进行这些更新直到收敛。

现在，让我们熟悉一下梯度下降是如何工作的。我们在梯度下降法中看到的一个例子是它可以得到局部最小值而不是全局最小值。全局最小值是否意味着成本函数J在所有可能点中可能值最小的点。

![[Pasted image 20230202210039.png]]

你可能还记得这个，它看起来像一个户外公园，有几座小山。这个函数有多个局部最小值。根据初始化参数w和b的位置，你可以得到不同的局部最小值。

![[Pasted image 20230202210120.png]]

但结果是，当你用平方误差成本函数进行线性回归时，成本函数不会也永远不会有多个局部最小值。它只有一个全局最小值，因为它是碗状的。专业术语是这个成本函数是一个凸函数。非正式地说，凸函数是碗形函数，它不能有任何局部最小值，而只有一个全局最小值。当你在一个凸函数上实现梯度下降时，一个很好的特性是，只要你选择了合适的学习率，它总是会收敛到全局最小值。

## 4.3 运行梯度下降

让我们看看在线性回归中使用梯度下降会发生什么。

![[Pasted image 20230202210354.png]]

左上方是模型和数据的图右上方是成本函数的等高线图底部是相同成本函数的表面图。通常w和b都被初始化为0，但是在这个演示中，让w初始化为-0.1,b初始化为900。所以这对应于f(x) = -0.1x + 900。

![[vavap-omtyr.gif|375]]

现在，如果我们用梯度下降法进行一步，我们最终会从成本函数的这一点移动到右边的这一点，注意到直线的拟合也发生了一些变化。

当采取更多这样的步骤时，每次更新的cost都会降低。所以参数w和b沿着这个轨迹。

如果你看左边，你会得到相应的直线拟合,它越来越适合数据直到我们达到全局最小值。全局最小值对应于这个直线拟合，这是一个相对较好的数据拟合。这种梯度下降过程称为批量梯度下降,指的是在梯度下降的每一步中，我们都要查看所有的训练示例，而不仅仅是训练数据的子集。

![[Pasted image 20230202211148.png]]

所以在计算导数时，批量梯度下降是在每次更新时查看整批训练示例。还有其他版本的梯度下降不关注整个训练集，而是在每个更新步骤中关注训练数据的较小子集。但是我们将用批量梯度下降来进行线性回归。