# 1. 多变量回归（Regression with multiple input variables）

## 1.1 多元特征（Multiple features）

![[Pasted image 20230203114758.png]]

在线性回归的原始版本中，你只有一个特征x，房子的大小。你可以预测y，房子的价格。模型是$f_{w,b} (x) = wx + b$。

![[Pasted image 20230203115244.png]]

但现在，如果你不仅有房子的大小作为预测价格的特征，而且还知道卧室的数量、楼层的数量和房子的使用年限，会怎样呢？这似乎能给你更多信息来预测价格。

为了简单起见，我们引入更多的符号。我们写$x_j$来表示特征列表。我将用小写的n来表示特征的总数。和前面一样，我们将使用$x^i$来表示第i个训练示例。这里$x^i$实际上是一个由四个数字组成的列表，或者有时我们称它为一个向量，它包含了第i个训练例子的所有特征。

为了指代第i个训练示例中的特定特征，我将写$x^i_j$。有时候为了强调$x^2$不是一个数字而是一个向量的数字列表，我们会在上面画一个箭头来直观地表示它是一个向量。它们有时只是用来强调这是一个向量而不是一个数字。


现在我们有了多个特性，让我们来看看模型是什么样子的：

![[Pasted image 20230203115540.png]]

模型将是，$f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$。

具体来说，对于房价预测，一种可能的模型可能是，我们估计房子的价格为0.1倍$x_1$(房子的大小)，加上4倍$x_2$(卧室数量)，加上10倍$x_3$(楼层数)，减去2倍$x_4$(房子的使用年限加80年)。

我们接下来要做的是引入一些符号用更简单但等价的方式重写这个表达式：



1. 我们把W定义为一个数字列表它列出了参数$w_1 w_2 w_3，... , w_n$。在数学中，这叫做向量。
2. 接下来，和前面一样，b是一个单独的数字，而不是一个向量。
3. 所以这个向量w和这个数字b是模型的参数。


这个模型现在可以更简洁地写成$$f (x) =\vec {w} \cdot \vec {x} + b$$
**这个点积是什么？**

w和x的两个向量的点积，是通过检查对应的数字对来计算的：$$w_1 \cdot x_1 + ... + w_n \cdot x_n $$最后+b。

这种具有多个输入特征的线性回归模型的名称是**多元线性回归**。这与单变量回归相反，单变量回归只有一个特征。

## 1.2 向量化（Vectorization）

当你在实现一个学习算法时，使用向量化既可以使你的代码更短，也可以使它运行得更高效。学习如何编写向量化代码将允许您利用现代数值线性代数库，以及甚至GPU硬件，代表图形处理单元。这是一种客观设计来加速计算机图形处理的硬件，但在编写向量化代码时，它也能帮助你更快地执行代码。

让我们看一个向量化的具体例子：
$$\vec{w} = [w_1 w_2 w_3] $$
b is a number
$$\vec{x} = [x_1 x_2 x_3] $$

这是一个有参数w和b的例子，其中w是一个有三个数字的向量，你也有一个特征x的向量，也有三个数字。这里n等于3。

### 1.2.1 无向量化的两种方式

在Python代码中，可以使用这样的数组定义这些变量w、b和x：
```python
w = np.array([1.0, 2.5, -3.3])
b = 4
x = np.array([10, 20, 30])
```
在这里，我实际上使用的是Python中名为NumPy的数值线性代数库，这是迄今为止Python和机器学习中使用最广泛的数值线性代数库。因为在Python中，在数组中计数时对数组的索引从0开始，您将使用$w[0]$访问w的第一个值。这里的索引是从0,1到2，而不是从1,2到3。类似地，要访问x的各个特征，您将使用x0、x1和x2。

现在，让我们看看一个没有向量化的实现来计算模型的预测：
$$f_{\vec w ,b}(\vec x) = w_1x_1 + w_2x_2 + w_3x_3 + b$$

在编码中，它看起来是这样的：
```python
f = w[0] * x[0] + 
	w[1] * x[1] + 
	w[2] * x[2] + b 
```

但是如果n不是3，而是100或者10万，这样写对于你的代码和你的计算机来说都是低效的。

还有另一种方法。不用向量化而是使用for循环。在数学中，你可以使用求和运算符将j从1到n的$w_j$和$x_j$的所有乘积相加，最后加上b。
$$f_{\vec w ,b}(\vec x) = \sum_{j=1}^nw_jx_j + b$$

```python
f = 0
for j in range(n):
	f += w[j] * x[j]
f += b
```


虽然好一些了，但仍然不是很有效。

### 1.2.2 向量化的方式

现在，让我们看看如何使用向量化来做到这一点。
$$f (x) =\vec {w} \cdot \vec {x} + b$$

这是函数f的数学表达式，现在你可以用一行代码来实现它：
```python
f = np.dot(w,x) + b
```

这个NumPy点函数是两个向量之间点积运算的向量化实现，特别是当n很大时，它将比前面两个代码示例运行得快得多。

向量化实际上有两个明显的好处：
1. 它使代码更短
2. 它比没有使用向量化的实现快得多

向量化实现要快得多的原因是：**NumPy点函数能够在你的计算机中使用并行硬件**，无论你是在普通计算机上运行，还是在普通计算机CPU上运行，或者如果你使用GPU，图形处理器单元，它通常用于加速机器学习工作。

### 1.2.3 多元线性回归的梯度下降

![[Pasted image 20230203133503.png]]
让我们快速回顾一下多元线性回归是什么样的。我们有参数$w_1$到$w_n$和b，这是单独的参数，这样w就是一个长度为n的向量。我们只需要把这个模型的参数看作一个向量w，还有b，其中b仍然是一个和以前一样的数。使用向量表示法，我们可以把模型写成$f_w b (x) = \vec{w} \cdot {x} + b$。

我们的代价函数可以定义为$J (w_1,...,w_n, b)$，J现在接受向量w和数字b的输入，并返回一个数字。
我们要反复更新每个参数，我们把它写成向量的形式。

![[Pasted image 20230203133853.png]]

让我们看看当你实现梯度下降时这是什么样子。我们会看到，梯度下降在有多个特征时和只有一个特征时略有不同。对于多元线性回归，我们有从1到n的J，所以我们会更新参数$w_1 w_2$，一直到$w_n$，然后像以前一样，我们会更新b。如果你实现了这个，你会得到多元回归的梯度下降。

这就是多元回归的梯度下降。

我想简单地讲一下线性回归中求w和b的另一种方法。这种方法叫做**法方程**。

梯度下降是一种很好的方法来最小化成本函数J来求w和b，还有一种算法只适用于线性回归，被称为**法方程**，它可以使用高级线性代数库在一个目标中求解w和b，而不需要迭代。

法方程法的一些缺点是;首先，与梯度下降不同的是，它不能推广到其他学习算法，同时计算速度也相当慢。但如果你使用一个成熟的机器学习库并调用线性回归，那么在后端就有可能使用它来求解w和b。

需要注意的是，一些机器学习库可能在后端使用这种复杂的方法来求解w和b。但对于大多数学习算法，包括你自己如何实现线性回归，梯度下降提供了更好的方法来完成这项工作。

# 2. 梯度下降法的实践（Gradient descent in practice）

## 2.1 特征缩放（Feature scaling）

本节会看到一种叫做特征缩放的技术，它可以使梯度下降运行得更快。

让我们先来看看一个特征的大小(即该特征的数字有多大)与其相关参数的大小之间的关系：

作为一个具体的例子，让我们用两个特征来预测房子的价格：x1房子的大小和x2卧室的数量。

![[Pasted image 20230203134915.png]]

假设x1一般在300平方英尺到2000平方英尺之间。数据集中x2的范围从0到5间卧室。在这个例子中，x1的取值范围相对较大而x2的取值范围相对较小。现在让我们以一个2000平方英尺的房子为例，有五个卧室，价格是50万或50万美元。

为了便于讨论，假设w1是50，w2是0.1，b是50。在这种情况下，以千美元计的估价是略多于1亿美元，这显然与50万美元的实际价格相差甚远。所以对于w1和w2，这不是一个很好的参数选择。

现在让我们来看看另一种可能性。假设w1和w2是反过来的。w1是0.， w2是50，b是50。在这个w1和w2的选择中，w1相对较小，w2相对较大，50远大于0.1。这里预测价格是50万美元，这是一个更合理的估计，而且恰好与房子的真实价格相同。

这说明了什么？

![[Pasted image 20230203135255.png]]

让我们看看成本函数在等高线图中的样子。其中水平轴的范围更窄，比如在0到1之间，而垂直轴的范围更大，比如在10到100之间。所以轮廓形成椭圆形或椭圆，它们一边短，另一边长。这是因为w1的一个很小的变化会对估计价格产生很大的影响，这对成本J产生很大的影响，因为w1往往会乘以一个很大的数字，面积和平方英尺。相比之下，w2需要更大的变化才能使预测发生很大变化。因此，对w2的小改变，不会对成本函数产生太大影响。

![[Pasted image 20230203135410.png]]

这给我们带来了什么？如果你按原样使用你的训练数据，梯度下降在最终找到全局最小值之前可能会来回反弹很长一段时间。在这种情况下，一个有用的方法是缩放特性。这意味着对训练数据进行一些转换，使得x1的范围从0到1,x2的范围也从0到1。

**关键是重新缩放x1和x2现在都对彼此取了可比较的值范围**。如果你对一个成本函数进行梯度下降，用这个变换后的数据重新缩放x1和x2，那么轮廓会看起来更像这样更像圆。梯度下降法可以更直接地找到全局最小值。

概括一下，**当你有不同的特征值范围非常不同时，它会导致梯度下降运行缓慢，但可以重新缩放不同的特征，使它们都有相当的值范围。

具体应该如何做呢？