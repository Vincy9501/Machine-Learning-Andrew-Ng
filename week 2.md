# 1. 多变量回归（Regression with multiple input variables）

## 1.1 多元特征（Multiple features）

![[Pasted image 20230203114758.png]]

在线性回归的原始版本中，你只有一个特征x，房子的大小。你可以预测y，房子的价格。模型是$f_{w,b} (x) = wx + b$。

![[Pasted image 20230203115244.png]]

但现在，如果你不仅有房子的大小作为预测价格的特征，而且还知道卧室的数量、楼层的数量和房子的使用年限，会怎样呢？这似乎能给你更多信息来预测价格。

为了简单起见，我们引入更多的符号。我们写$x_j$来表示特征列表。我将用小写的n来表示特征的总数。和前面一样，我们将使用$x^i$来表示第i个训练示例。这里$x^i$实际上是一个由四个数字组成的列表，或者有时我们称它为一个向量，它包含了第i个训练例子的所有特征。

为了指代第i个训练示例中的特定特征，我将写$x^i_j$。有时候为了强调$x^2$不是一个数字而是一个向量的数字列表，我们会在上面画一个箭头来直观地表示它是一个向量。它们有时只是用来强调这是一个向量而不是一个数字。


现在我们有了多个特性，让我们来看看模型是什么样子的：

![[Pasted image 20230203115540.png]]

模型将是，$f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$。

具体来说，对于房价预测，一种可能的模型可能是，我们估计房子的价格为0.1倍$x_1$(房子的大小)，加上4倍$x_2$(卧室数量)，加上10倍$x_3$(楼层数)，减去2倍$x_4$(房子的使用年限加80年)。

我们接下来要做的是引入一些符号用更简单但等价的方式重写这个表达式：



1. 我们把W定义为一个数字列表它列出了参数$w_1 w_2 w_3，... , w_n$。在数学中，这叫做向量。
2. 接下来，和前面一样，b是一个单独的数字，而不是一个向量。
3. 所以这个向量w和这个数字b是模型的参数。


这个模型现在可以更简洁地写成$$f (x) =\vec {w} \cdot \vec {x} + b$$
**这个点积是什么？**

w和x的两个向量的点积，是通过检查对应的数字对来计算的：$$w_1 \cdot x_1 + ... + w_n \cdot x_n $$最后+b。

这种具有多个输入特征的线性回归模型的名称是**多元线性回归**。这与单变量回归相反，单变量回归只有一个特征。

## 1.2 向量化（Vectorization）

当你在实现一个学习算法时，使用向量化既可以使你的代码更短，也可以使它运行得更高效。学习如何编写向量化代码将允许您利用现代数值线性代数库，以及甚至GPU硬件，代表图形处理单元。这是一种客观设计来加速计算机图形处理的硬件，但在编写向量化代码时，它也能帮助你更快地执行代码。

让我们看一个向量化的具体例子：
$$\vec{w} = [w_1 w_2 w_3] $$
b is a number
$$\vec{x} = [x_1 x_2 x_3] $$

这是一个有参数w和b的例子，其中w是一个有三个数字的向量，你也有一个特征x的向量，也有三个数字。这里n等于3。

### 1.2.1 无向量化的两种方式

在Python代码中，可以使用这样的数组定义这些变量w、b和x：
```python
w = np.array([1.0, 2.5, -3.3])
b = 4
x = np.array([10, 20, 30])
```
在这里，我实际上使用的是Python中名为NumPy的数值线性代数库，这是迄今为止Python和机器学习中使用最广泛的数值线性代数库。因为在Python中，在数组中计数时对数组的索引从0开始，您将使用$w[0]$访问w的第一个值。这里的索引是从0,1到2，而不是从1,2到3。类似地，要访问x的各个特征，您将使用x0、x1和x2。

现在，让我们看看一个没有向量化的实现来计算模型的预测：
$$f_{\vec w ,b}(\vec x) = w_1x_1 + w_2x_2 + w_3x_3 + b$$

在编码中，它看起来是这样的：
```python
f = w[0] * x[0] + 
	w[1] * x[1] + 
	w[2] * x[2] + b 
```

但是如果n不是3，而是100或者10万，这样写对于你的代码和你的计算机来说都是低效的。

还有另一种方法。不用向量化而是使用for循环。在数学中，你可以使用求和运算符将j从1到n的$w_j$和$x_j$的所有乘积相加，最后加上b。
$$f_{\vec w ,b}(\vec x) = \sum_{j=1}^nw_jx_j + b$$

```python
f = 0
for j in range(n):
	f += w[j] * x[j]
f += b
```


虽然好一些了，但仍然不是很有效。

### 1.2.2 向量化的方式

现在，让我们看看如何使用向量化来做到这一点。
$$f (x) =\vec {w} \cdot \vec {x} + b$$

这是函数f的数学表达式，现在你可以用一行代码来实现它：
```python
f = np.dot(w,x) + b
```

这个NumPy点函数是两个向量之间点积运算的向量化实现，特别是当n很大时，它将比前面两个代码示例运行得快得多。

向量化实际上有两个明显的好处：
1. 它使代码更短
2. 它比没有使用向量化的实现快得多

向量化实现要快得多的原因是：**NumPy点函数能够在你的计算机中使用并行硬件**，无论你是在普通计算机上运行，还是在普通计算机CPU上运行，或者如果你使用GPU，图形处理器单元，它通常用于加速机器学习工作。

### 1.2.3 多元线性回归的梯度下降

![[Pasted image 20230203133503.png]]
让我们快速回顾一下多元线性回归是什么样的。我们有参数$w_1$到$w_n$和b，这是单独的参数，这样w就是一个长度为n的向量。我们只需要把这个模型的参数看作一个向量w，还有b，其中b仍然是一个和以前一样的数。使用向量表示法，我们可以把模型写成$f_w b (x) = \vec{w} \cdot {x} + b$。

我们的代价函数可以定义为$J (w_1,...,w_n, b)$，J现在接受向量w和数字b的输入，并返回一个数字。
我们要反复更新每个参数，我们把它写成向量的形式。

![[Pasted image 20230203133853.png]]

让我们看看当你实现梯度下降时这是什么样子。我们会看到，梯度下降在有多个特征时和只有一个特征时略有不同。对于多元线性回归，我们有从1到n的J，所以我们会更新参数$w_1 w_2$，一直到$w_n$，然后像以前一样，我们会更新b。如果你实现了这个，你会得到多元回归的梯度下降。

这就是多元回归的梯度下降。

我想简单地讲一下线性回归中求w和b的另一种方法。这种方法叫做**法方程**。

梯度下降是一种很好的方法来最小化成本函数J来求w和b，还有一种算法只适用于线性回归，被称为**法方程**，它可以使用高级线性代数库在一个目标中求解w和b，而不需要迭代。

法方程法的一些缺点是;首先，与梯度下降不同的是，它不能推广到其他学习算法，同时计算速度也相当慢。但如果你使用一个成熟的机器学习库并调用线性回归，那么在后端就有可能使用它来求解w和b。

需要注意的是，一些机器学习库可能在后端使用这种复杂的方法来求解w和b。但对于大多数学习算法，包括你自己如何实现线性回归，梯度下降提供了更好的方法来完成这项工作。

# 2. 梯度下降法的实践（Gradient descent in practice）

## 2.1 特征缩放（Feature scaling）

本节会看到一种叫做特征缩放的技术，它可以使梯度下降运行得更快。

让我们先来看看一个特征的大小(即该特征的数字有多大)与其相关参数的大小之间的关系：

作为一个具体的例子，让我们用两个特征来预测房子的价格：x1房子的大小和x2卧室的数量。

![[Pasted image 20230203134915.png]]

假设x1一般在300平方英尺到2000平方英尺之间。数据集中x2的范围从0到5间卧室。在这个例子中，x1的取值范围相对较大而x2的取值范围相对较小。现在让我们以一个2000平方英尺的房子为例，有五个卧室，价格是50万或50万美元。

为了便于讨论，假设w1是50，w2是0.1，b是50。在这种情况下，以千美元计的估价是略多于1亿美元，这显然与50万美元的实际价格相差甚远。所以对于w1和w2，这不是一个很好的参数选择。

现在让我们来看看另一种可能性。假设w1和w2是反过来的。w1是0.， w2是50，b是50。在这个w1和w2的选择中，w1相对较小，w2相对较大，50远大于0.1。这里预测价格是50万美元，这是一个更合理的估计，而且恰好与房子的真实价格相同。

这说明了什么？

![[Pasted image 20230203135255.png]]

让我们看看成本函数在等高线图中的样子。其中水平轴的范围更窄，比如在0到1之间，而垂直轴的范围更大，比如在10到100之间。所以轮廓形成椭圆形或椭圆，它们一边短，另一边长。这是因为w1的一个很小的变化会对估计价格产生很大的影响，这对成本J产生很大的影响，因为w1往往会乘以一个很大的数字，面积和平方英尺。相比之下，w2需要更大的变化才能使预测发生很大变化。因此，对w2的小改变，不会对成本函数产生太大影响。

![[Pasted image 20230203135410.png]]

这给我们带来了什么？如果你按原样使用你的训练数据，梯度下降在最终找到全局最小值之前可能会来回反弹很长一段时间。在这种情况下，一个有用的方法是缩放特性。这意味着对训练数据进行一些转换，使得x1的范围从0到1,x2的范围也从0到1。

**关键是重新缩放x1和x2现在都对彼此取了可比较的值范围**。如果你对一个成本函数进行梯度下降，用这个变换后的数据重新缩放x1和x2，那么轮廓会看起来更像这样更像圆。梯度下降法可以更直接地找到全局最小值。

概括一下，**当你有不同的特征值范围非常不同时，它会导致梯度下降运行缓慢，但可以重新缩放不同的特征，使它们都有相当的值范围。

具体应该如何做呢？

### 2.1.1 实现的三种方式（最大值、均值归一化、z-score）

如果x1的范围是3- 2000，得到x1的缩放版本的一种方法是将每个原始的x1/2000，即范围的最大值。刻度x1的范围为0.15-1。类似地，由于x2的范围是0-5，您可以通过取每个原始x2/5来计算x2的缩放版本，这也是最大值。所以x2的范围是0-1。

![[Pasted image 20230204130907.png]]

如果你在图上画出x1和x2的比例，它可能是这样的。

除了除以最大值，你还可以做均值归一化。

![[Pasted image 20230204131246.png]]

这看起来是这样的：你从最初的特征开始，然后重新缩放它们，使它们都以0为中心。以前它们只有大于0的值，现在它们有正负两种值，通常介于- 1和+ 1之间。要计算x1的均值归一化，首先求出训练集中的均值，也叫x1的均值，我们称其均值为$\mu{1}$。

例如，你可能会发现特征1 $\mu{1}$的平均值是600平方英尺。取每个x1，减去$\mu{1}$的平均值，然后除以2000 - 300的差值，其中2000是最大值，300是最小值，这样做，就得到了标准化的x_1范围在- 0.18-0.82之间。


还有最后一种常见的缩放方法叫做Z-score归一化。要实现Z-score归一化，需要计算每个特征的标准偏差。正态分布或钟形曲线，有时也叫高斯分布，这就是正态分布的标准差。

1. 首先计算平均值$\mu 1$，以及标准偏差$\sigma 1$。

例如，特征1的标准差为450，均值为600，然后z分数归一化x1，取每个x1，减去$\mu_1$，然后除以标准差$\sigma 1$。你可能会发现z分数归一化x1现在的范围是- 0.67-3.1。

类似地，如果你计算第二个特征的标准偏差为1.4，均值为2.3，那么你可以计算标准化的z分数在- 1.6-1.9之间。如果你把训练数据在归一化的x_1和x_2上画在图上，它可能是这样的。

![[Pasted image 20230204131537.png]]

作为一个经验法则，当执行特征缩放时，对于每个特征x，你可能想要让特征的范围从- 1到+ 1左右。

在下一个视频中，我们来看看，然后在下一个视频中，这将导致。

## 2.2 识别梯度下降是否是收敛的（Checking gradient descent for convergence）

当运行梯度下降时，你如何判断它是否收敛?也就是说，它是否能帮助你找到接近代价函数全局最小值的参数。

让我们来看看。这是梯度下降法则。其中一个关键的选择是学习率$\alpha$的选择。梯度下降的工作是找到能使成本函数J最小化的参数w和b，我经常做的是画出成本函数J，它是在训练集上计算出来的，我在梯度下降的每次迭代中画出J的值。

![[Pasted image 20230204132322.png]]

在这个图中，横轴是到目前为止运行的梯度下降迭代的次数。你可能会得到这样的曲线。注意，横轴是梯度下降的迭代次数，而不是w或b这样的参数。这与之前的图表不同，在前面的图表中，**纵轴是成本J，横轴是w或b这样的单个参数。这条曲线也被称为学习曲线**。

具体地说，如果你看曲线上的这个点，这意味着在你运行梯度下降100次迭代之后，意味着同时更新参数100次，你得到了w和b的一些值。如果你计算w和b的代价J, w, b，你在100次迭代之后得到的，你得到了这个成本J的值，这就是纵轴上的这个点。这个点对应于200次梯度下降迭代后得到的参数J的值。查看这张图可以帮助您了解成本J在每次梯度下降迭代后的变化情况。如果梯度下降工作正常，那么在每次迭代之后，成本J应该会减少。如果J在一次迭代后增加，这意味着要么学习率选择得不好，通常意味着学习率太大，要么代码中可能存在错误。这部分可以告诉您的另一件有用的事情是，如果您查看这条曲线，当您达到300次迭代时，成本J将趋于平稳，不再减少太多。经过400次迭代，曲线看起来已经变平了。这意味着梯度下降或多或少收敛了，因为曲线不再下降。看看这条学习曲线，你可以试着找出梯度下降是否收敛。

顺便说一下，在不同的应用程序之间，梯度下降进行转换的迭代次数可能有很大差异。在一个应用程序中，它可能在30次迭代后收敛。对于不同的应用程序，可能需要1,000或100,000次迭代。事实证明，要提前知道梯度下降需要收敛多少次迭代是非常困难的，这就是为什么你可以创建这样一个图形，一个学习曲线。试着找出你什么时候可以开始训练你的特定模型。


另一种决定你的模型何时完成训练的方法是**自动收敛测试**。

这是希腊字母$\epsilon$。我们设为表示一个小数字的变量，比如0.001或10^-3。如果成本J在一次迭代中减少的小于这个数，那么你很可能在曲线的这部分变平了。就像你在左边看到的那样你就可以宣布收敛。记住，收敛，希望在你找到参数w和b的情况下它们接近于J的最小值，我通常发现选择正确的阈值是相当困难的。实际上，我倾向于看左边这个图，而不是依赖自动收敛测试。


## 2.3 如何为梯度下降选择一个好的学习率的讨论

选择合适的学习率，你的学习算法会运行得更好。如果它太小，它会运行得非常慢，如果它太大，它甚至可能不会收敛。如果您绘制了多次迭代的成本，并注意到成本有时上升，有时下降，您应该将其视为梯度下降不能正常工作的明显迹象。这可能意味着代码中有一个错误。或者有时它可能意味着你的学习速度太大。

![[Pasted image 20230204133509.png]]

纵轴是成本函数J，横轴表示一个参数。有时您可能会看到成本在每次迭代后持续增加，就像左下角的图一样。这也可能是由于学习率过大，可以通过选择较小的学习率来解决。但像这样的学习率也可能是代码可能被破坏的迹象。例如右上图，这是因为右边公式的缘故。

正确实现梯度下降的一个调试技巧是，**在足够小的学习率下，成本函数应该在每次迭代中减**小。如果梯度下降不起作用，我经常做的一件事是**把$\alpha$设置为一个很小的数字**，看看这**是否会导致每次迭代的成本减少**。如果$\alpha$值设置得非常小，J并没有在每次迭代中减少，而是有时增加，那么这通常意味着代码中的某个地方存在bug。


请注意，非常小的$\alpha$值对于实际训练学习算法来说并不是最有效的选择。当我运行梯度下降时，我通常会尝试一个学习率$\alpha$的值范围。我可以从0.001的学习率开始，我也可以尝试10倍的学习率，比如0.01和0.1，等等。对于$\alpha$的每个选择，您可能只对少数几个迭代运行梯度下降，并将J绘制为迭代次数的函数，在尝试了几个不同的值后，您可能会选择$\alpha$的值，该值似乎可以快速降低学习率。实际上，我所做的是尝试这样一个值的范围。在尝试0.001之后，我将学习率提高三倍，达到0.003。在那之后，我将尝试0.01，它仍然是0.003的三倍。所以这些大概是在尝试梯度下降每个$\alpha$值大约是前一个值的三倍。我要做的是尝试一个范围的值，直到我找到一个太小的值，然后也要确保我找到一个太大的值。我会慢慢地选择最大的学习率，或者稍微小于我找到的最大合理值。当我这样做时，它通常会给我的模型一个很好的学习率。


现在，还有一些想法可以用来使多元线性回归更加强大。这就是选择自定义特征，这也将允许你对数据拟合曲线，而不仅仅是直线。

## 2.4 特征工程（Feature engineering）

特征的选择对学习算法的性能有很大的影响。事实上，对于许多实际应用来说，选择或输入正确的特征是使算法工作良好的关键步骤。

让我们通过回顾预测房屋价格的例子来了解特征工程。

![[Pasted image 20230204135240.png]]

假设每个房子都有两个功能。x1为房屋所建地块的地块大小的宽度。这在现实中也被称为地块的正面，第二个特征，x2，是地块大小的深度，我们假设房子建在矩形的土地上。给定这两个特征，你可以建立一个这样的模型：
$$f (x)=w_1x_1 + w_2x_2 + b$$
这个模型可能还行。但是这里有另一种选择，您可以选择一种不同的方式来使用模型中的这些功能，这可能会更有效。您可能会注意到：
$$area = frontage × depth$$
你可能会有一种直觉，认为土地面积比正面和深度作为单独的特征更能预测价格。你可以定义一个新特征，x3，$x_3 = x_1x_2$。这个新特征x3等于这块土地的面积。有了这个特征，你就可以得到一个模型$$f_{\vec{w},b}(\vec {x})=w_1x_1 + w_2x_2 + w_3x_3 +b$$我们刚刚所做的，**创建一个新特征，是所谓的特征工程的一个例子**，在这个过程中，你可能会利用你对问题的知识或直觉来**设计新的特征**，通常是通过转换或结合问题的原始特征，**以便让学习算法更容易做出准确的预测**。

## 2.5 多项式回归（Polynomial regression）

到目前为止，我们只是用直线来拟合数据。让我们利用多元线性回归和特征工程的思想来提出一种叫做多项式回归的新算法，它可以让你将曲线，非线性函数，与你的数据拟合。

![[Pasted image 20230204135621.png]]

假设你有一个房屋数据集是这样的，特征x是平方英尺的大小。这条直线看起来不太适合这个数据集。也许你想拟合一条曲线，也许是一个二次函数像这样的数据，它的大小是x，也包括x²，它的大小是2的幂。也许这能让你更好地拟合数据。但是你可能会觉得二次函数模型没有意义因为二次函数最终会回来。好吧，我们并不指望房价会随着面积的增加而下降。大房子看起来通常应该更贵。然后你可以选择一个三次函数。

现在我们不仅有x的平方，还有x的立方。

这两个都是多项式回归的例子，因为你把你的可选特征x，取2或3的幂或者其他幂。在立方函数的情况下，第一个特征是大小，第二个特征是大小的平方，第三个特征是大小的立方。

我还想指出一件事，那就是如果你创造的特征是这些幂就像原始特征的平方，那么特征缩放就变得越来越重要。如果房子的面积在1- 1000平方英尺之间，那么第二个特征，也就是面积的平方英尺，就会在1到100万平方英尺之间，而第三个特征，也就是面积的立方英尺，就会在10到10亿平方英尺之间。这两个特征，x平方和x立方，与原始特征x相比，具有非常不同的值范围。

如果你使用梯度下降，应用特征缩放来使你的特征进入可比的值范围是很重要的。

![[Pasted image 20230204135911.png]]

这是最后一个示例，说明如何真正拥有广泛的特性选择。另一个合理的选择是使用x的平方根，你的模型可能看起来像$$f_{\vec{w},b}(\vec {x})=w_1x + w_2\sqrt x ++b$$这将是另一种可能适用于此数据集的功能选择。您可能会问自己，我如何决定使用哪些特征？在本专业课程的第二门课程中，您将了解如何选择包含或不包含这些特性的不同特性和不同模型，并有一个测量这些不同模型性能的过程，以帮助您决定包含或不包含哪些特征。

现在，我只想让你知道你可以选择使用什么特性。通过使用**特征工程**和**多项式函数**，你可以为你的数据得到一个更好的模型。

# 3. 总结

- 多元线性回归是具有多个输入特征的线性回归模型。
- 向量化能让代码更短。
- 要使梯度下降法运行更快，可以使用特征缩放，实现的三种方式是除以最大值、均值归一化和z-score法。
- 要判断梯度下降是否是收敛的，可以画出成本函数对迭代次数的图，也就是学习曲线。另一种方法是自动收敛测试，但不推荐使用。
- 要使梯度下降选择一个好学习率，需要结合学习曲线和成本函数对w的图像，在足够小的学习率下，成本函数应该在每次迭代中减小。
- 特征工程是通过设计新特征，以让学习算法更容易做出准确的预测。
- 多项式回归也可以让数据得到一个更好的模型。