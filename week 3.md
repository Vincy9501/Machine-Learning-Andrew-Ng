# 1. 用逻辑回归分类（Classification with logistic regression）

## 1.1 动机（motivation）

上周我们学习了线性回归，它预测一个数字。本周，您将学习分类，其中输出变量y只能取少数几个可能值中的一个，而不是无限范围内的任何数字。线性回归并不是一个很好的分类算法。让我们来看看原因，这将引导我们进入另一种叫做逻辑回归的算法。

例子：
1. 判断一封电子邮件是否是垃圾邮件
2. 判断在线金融交易是否存在欺诈
3. 将肿瘤划分为恶性和非恶性

在这些问题中，你想要预测的变量只能是两个可能值中的一个。这种只有两种可能输出的分类问题称为二元分类。二进制这个词指的是只有两种可能的类或两种可能的范畴。在这些问题中，我将相对交替地使用**类**和**范畴**这两个术语。它们的意思基本上是一样的。

我们经常用no或yes来表示从句，有时也用false或true来表示，通常用数字0或1来表示。遵循计算机科学中的常见惯例，0表示假，1表示真。我通常会用数字0和1来表示答案y，因为这最适合我们想要实现的学习算法类型。

如何构建一个分类算法？

![[Pasted image 20230205134314.png]]

这是一个训练集的例子，用于分类肿瘤是否为恶性

现在，你可以在这个训练集上尝试的一件事是应用你已经知道的算法：线性回归，并试图拟合一条直线的数据。如果你这样做，直线可能是这样的：

![[Pasted image 20230205134359.png]]

所有的数字都在0到1之间或者小于0或者大于1。但这里我们想要预测类别，您可以尝试的一件事是选择一个阈值，例如0.5。因此，如果模型输出的值低于0.5，那么您就可以预测等于零或不是恶性的。如果模型输出的数字等于或大于0.5，则预测Y等于1或恶性。


请注意，阈值0.5在这一点与最佳拟合直线相交。如果你在这里画一条垂直线，左边的所有东西都预测y等于0。右边所有的结果都是y = 1。现在，对于这个特定的数据集线性回归似乎可以做一些合理的事情。但现在让我们看看如果你的数据集还有一个训练示例会发生什么。右边这边。我们再延伸一下横轴。注意，这个训练示例不应该真正改变数据点的分类方式。我们刚才画的这条垂直分界线仍然是有意义的，作为分界线，比这小的肿瘤应该归为零。大于此值的肿瘤应该归为1。

但是一旦你添加了极端的训练示例，比如当肿瘤很大时我们希望算法将其归类为恶性。但是我们会得到一条糟糕的线。

因此不推荐线性回归在分类问题中使用。

## 1.2 逻辑回归（Logistic regression）

让我们来谈谈逻辑回归，这可能是世界上使用最广泛的分类算法。让我们继续讨论肿瘤是否恶性的分类例子。我们将用标签1或者是阳性类别来代表恶性肿瘤，用0或者否和阴性例子来代表良性肿瘤。

![[Pasted image 20230205135054.png]]

这是一个数据集的图表，横轴是肿瘤大小，垂直轴只有0和1的值。线性回归并不是解决这个问题的好算法。相比之下，我们使用逻辑回归拟合一个像这样的曲线，s型曲线到这个数据集。

在这个例子中，如果一个病人x轴上标注的这么大的肿瘤，那么算法将输出0.7，这表明它更接近或更可能是恶性的。

为了建立逻辑回归算法，我想描述一个重要的数学函数，它被称为Sigmoid函数，有时也被称为逻辑函数。Sigmoid函数是这样的：

![[Pasted image 20230205135151.png]]

请注意，上下两x轴是不同的。上图x轴是肿瘤大小，所以都是正数。而在下图中，这里是0，横轴有正负两种值横轴标为z。

所以Sigmoid函数的输出值在0到1之间。函数表达式为：

$$g(z) = \frac1{1+e^{-z}} \qquad 0<g(z)<1$$

这就是为什么sigmoid函数有这样的形状它开始时非常接近于0然后慢慢地增加或增长到1。同样，在Sigmoid函数中，当z = 0时，g (z)等于0.5。

现在，让我们用这个来建立逻辑回归算法。我们分两步来做：

1. 线性回归函数可以定义$f_{\vec{w},b}(\vec x) = \vec w \cdot \vec x + b$，我们把这个值存储在一个变量中，称之为z。也就是说$z = \vec w \cdot \vec x + b$。
2. 取这个z的值并将它传递给Sigmoid函数，也叫逻辑函数g，现在g (z)输出一个由这个公式计算出来的值，它将在0到1之间。

当你把这两个方程放在一起，它们就给出了逻辑回归模型：
$$f_{\vec{w},b}(\vec x) = \frac 1 {1 + e^{-(\vec w \cdot \vec x + b)}}$$它所做的是输入一个特征或一组特征X然后输出一个0到1之间的数字。

我们回到肿瘤分类的例子。x是肿瘤大小，y是0或1。如果你有一个病人，她一定大小的肿瘤x输入这个模型获得的结果为0.7，这就意味着模型预测或者模型认为有70%的概率这个病人的y值等于1。换句话说，模型告诉我们，它认为病人的肿瘤有70%的几率是恶性的。

如果有一天你阅读研究论文或所有逻辑回归的博客，有时你会看到这个符号：
$$f_{\vec{w},b}(\vec x) =P(y=1|\vec x ;\vec w,b)$$这表示给定输入特征x、参数w和b，这里分号是用来表示w和b是影响计算的参数，在输入特征x的情况下y等于1的概率是多少。

## 1.3 决策边界（Decision boundary）

上一节，我们学习了逻辑回归模型。现在，让我们来看看决策边界，以更好地了解逻辑回归是如何计算这些预测的。

我们回想一下上一节中提到的，逻辑回归算法需要两步，第一步是让z=wx+b，第二步是把z带入sigmoid函数中。我们把结果解释为y = 1的概率，给定x，参数w和b，这可能是一个0.7或0.3的数字。

现在，如果你想学习预测的算法。y的值是0还是1 ?

你要做的一件事是设置一个阈值，超过这个阈值，你预测y是1，或者你设置y帽为预测值等于1，低于这个阈值，你可以说y帽，我的预测值等于0。一个常见的选择是选择阈值为0.5，这样如果f (x)大于或等于0.5，则预测y为1。我们把这个预测写成y帽等于1，或者如果f (x)小于0.5，那么预测y是0，换句话说，预测y帽等于0。

现在，让我们更深入地研究一下这个模型何时会预测到1。也就是g (z)什么时候大于等于0.5？这是一个Sigmoid函数，所以当z大于等于0时g (z)大于等于0.5。

那么z什么时候大于等于0 ? wx + b > 0 即可。

现在让我们来想象一下模型是如何进行预测的：

![[Pasted image 20230205142154.png]]

例如，这里有两个特征，x1和x2。红色的叉对应y = 1，蓝色的圆对应y = 0。逻辑回归模型将使用函数f (x) = g (z)进行预测。z现在是这个表达式：$$f_{\vec{w},b}(\vec x) = g(z) = g(w_1x_1+w_2x_2+b)$$在这个例子中，参数的值是w1 = 1，w2 = 1，b = - 3。现在让我们看看逻辑回归是如何进行预测的。我们来计算一下什么时候wx + b大于等于0什么时候wx + b小于0。要算出这个，有一条很有趣的直线，也就是当wx + b恰好等于0时，这条线也被称为**决策边界**。因为在这条线上，你对y = 0还是y = 1几乎保持中立。

对于我们的例子，这个决策边界就是$x_1$+$x_2$ - 3。$x_1$+$x_2$ - 3 = 0时的这条线就是决策边界。

现在让我们看一个更复杂的例子，其中决策边界不再是一条直线。和前面一样，叉表示类y = 1，小圆表示类y = 0。

![[Pasted image 20230205142501.png]]

z现在是这个表达式：$$f_{\vec{w},b}(\vec x) = g(z) = g(w_1x_1^2+w_2x_2^2+b)$$
通过选择这样的特征，多项式特征变成了逻辑回归。令：
$$z = w_1x_1^2+w_2x_2^2 - 1 = 0$$当$x_1²+ x_2²= 1$时，这个表达式等于0。

下一节，我们将从逻辑回归的成本函数开始，然后，弄清楚如何将梯度下降应用于它。

# 2. 逻辑回归的成本函数（Cost function for logistic regression）

## 2.1 逻辑回归的成本函数（Cost function for logistic regression）

请记住，成本函数为您提供了一种方法来衡量特定参数集与训练数据的拟合程度。从而给了你一种选择更好参数的方法。在这个视频中，我们将看看平方误差成本函数为什么不是逻辑回归的理想成本函数。我们将看看一个不同的成本函数，它可以帮助我们为逻辑回归选择更好的参数。

和前面一样，我们用m表示训练示例的数量。每个训练示例都有一个或多个特征，如肿瘤大小、患者年龄等，总共有n个特征。我们称这些特征为$x_1 ... x_n$。由于这是一个二进制分类任务，目标标签y只有两个值，要么是0，要么是1。最后，用该方程定义逻辑回归模型。

你要回答的问题是，给定这个训练集，如何选择参数w和b？


回忆一下线性回归，平方误差代价函数是$$J(\vec w, b) = \frac 1m \sum_{i=1}^m \frac 1 2(f_{(\vec w, b)}(\vec x^{(i)}-y^{(i)})^2$$成本函数看起来是一个凸函数或者碗形或锤形。梯度下降一步一步地收敛到全局最小值。

现在你可以用同样的成本函数来做逻辑回归。

如果$$f_{\vec{w},b}(\vec x) = \frac 1 {1 + e^{-(\vec w \cdot \vec x + b)}}$$然后用f (x)的值来画成本函数，就会像这样：

![[Pasted image 20230206110257.png]]

这意味着如果你尝试使用梯度下降法，有很多局部极小值你可以得到。事实证明，对于逻辑回归，这个平方误差成本函数不是一个好的选择。

梯度下降算法可以保证收敛到全局最小值。为了建立一个新的成本函数，我们将用它来进行逻辑回归。我要稍微改变一下成本函数J (w和b)的定义，特别是，如果你看这个和：
$$\frac 1 2(f_{(\vec w, b)}(\vec x^{(i)}-y^{(i)})^2$$
我们把这一项叫做单个训练例子中的损失。我要用大写的L来表示损失。

![[Pasted image 20230206111530.png]]

我们来看看为什么这个损失函数是有意义的。让我们先考虑y = 1的情况，然后画出这个函数来直观地了解这个损失函数是怎样的。损失函数衡量的是你在一个训练样本上做得有多好，它是通过把所有训练样本上的损失加起来得到的，成本函数衡量的是你在整个训练集上做得有多好。

![[Pasted image 20230206111610.png]]


f总是在0和1之间，因为逻辑回归的输出总是在0和1之间。函数中唯一相关的部分就是这部分，对应于f在0到1之间。让我们放大，仔细看看图表的这一部分。如果算法预测的概率接近1，而真实标签为1，则损失非常小。它几乎是0，因为你非常接近正确答案。现在继续以y = 1为例，如果算法的输出值是0.1如果它认为肿瘤是恶性的概率只有10%但y确实是1。

![[Pasted image 20230206112143.png]]

在这个图中，对应于y = 0，纵轴显示了不同f (x)值的损失值。当f为0或非常接近0时，损失也会非常小，这意味着如果真实标签为0，模型的预测非常接近0。

f (x)的值越大，损失就越大，因为预测值离真实值0越远。事实上，当预测接近1时，损失实际上接近无穷大。

回到肿瘤预测的例子，如果模型预测病人的肿瘤几乎肯定是恶性的，比如，99.9%的恶性概率，结果实际上不是恶性的，所以y = 0，那么我们对模型的损失非常大。

在y = 0的情况下，预测f (x)离y的真实值越远，损失越大。事实上，如果f (x)趋近于0，这里的损失就会变得很大，趋近于无穷。当真实标签为1时，算法强烈地激励不去预测太接近0的东西。

如果你能找到参数w和b的值，使这个最小化，那么你就有了一个很好的参数w和b的值集用于逻辑回归。

## 2.2 简化逻辑回归的成本函数（Simplified Cost Function for Logistic Regression）

这是我们在之前的视频中定义的逻辑回归的损失函数：

![[Pasted image 20230206112143.png]]

因为我们仍然在处理一个二元分类问题，y不是0就是1。因为y不是0就是1，不能取除0和1以外的任何值，我们可以用一种更简单的方法来写这个损失函数。

损失函数可以写成这样：

![[Pasted image 20230206112604.png]]

结果是这个方程，我们只用一行写出来，完全等价于上面这个更复杂的公式。使用这个简化的损失函数，让我们回过头来写出逻辑回归的成本函数。

![[Pasted image 20230206113009.png]]

这又是一个简化的损失函数。回想一下，成本J只是平均损失，整个训练集m个例子的平均损失。所以它有如下公式，如果你把上面的简化损失的定义代入，它就是1 / m乘以上面这一项的和。如果你把负号移到外面，那么你就得到了这个表达式，这是成本函数。

# 3. 梯度下降的实现（Gradient Descent Implementation）

为了拟合逻辑回归模型的参数，我们将尝试找到使成本函数J (w和b)最小的参数w和b的值，我们将再次应用梯度下降来做到这一点。
在本节我们将重点讨论如何找到参数w和b的一个好的选择。

![[Pasted image 20230206113448.png]]

这是通常的梯度下降算法，你反复更新每个参数。

把这些导数表达式代入这些项。这为逻辑回归提供了梯度下降。

![[Pasted image 20230206113614.png]]

现在，你可能会好奇一件有趣的事情是，这两个方程看起来很像我们之前得到的线性回归的平均值，所以你可能会想，线性回归实际上和逻辑回归是一样的吗?虽然这些方程看起来一样，但这不是线性回归的原因，是函数f (x)的定义变了。在线性回归中，f (x)等于，这是wx + b，但在逻辑回归中，f (x)被定义为应用于wx + b的sigmoid函数，虽然线性回归和逻辑回归的算法看起来是一样的，但实际上它们是两种非常不同的算法，因为f (x)的定义是不一样的。

# 4. 过拟合的问题（The Problem of Overfitting）

## 4.1 过拟合的问题（The Problem of Overfitting）

现在你已经看到了一些不同的学习算法，线性回归和逻辑回归。它们在许多任务中都能很好地工作。但有时在应用程序中，算法会遇到一个称为过拟合的问题，这可能会导致它的性能不佳。以及一个密切相关的，几乎相反的问题，称为欠拟合。

有一种称为正则化(regularization)的技术，它可以改善或者减少过度拟合问题。

**什么是过拟合？**

让我们回到最初用线性回归预测房价的例子。你想要预测价格作为房子大小的函数：

![[Pasted image 20230207113144.png|250]]

假设你的数据集是这样的，输入特征x是房子的大小，y是你试图预测房子价格的值。你可以做的一件事就是用线性函数来拟合这个数据。如果你这样做，你会得到一条符合数据的直线，但这不是一个很好的模型。从数据上看，很明显，随着房屋面积的增加，住房建设过程趋于平缓。

这个算法不太适合训练数据。用专业术语来说就是模型欠拟合训练数据。另一个术语是算法具有高偏差。在机器学习中，术语偏差有多重含义。基于性别或种族等特征检查学习算法的偏见是绝对关键的。但是**术语偏差**还有另一个技术意义，**就是如果算法不太适合数据，这意味着它甚至不能很好地适合训练集**。

![[Pasted image 20230207113526.png|250]]

如果你在有两个特征的数据上插入一个二次函数，那么当你拟合参数w1和w2时，你可以得到一条更好地拟合数据的曲线。**如果你是房地产经纪人，你想让你的学习算法做得很好，即使是在训练集之外的例子上，这就是泛化**。从技术上讲，我们说你希望你的学习算法能很好地泛化，这意味着即使是在它从未见过的全新例子上也能做出良好的预测。

![[Pasted image 20230207113630.png|250]]

如果你要用四阶多项式来拟合数据呢？
你可能会得到这样一条曲线。一方面，这似乎非常好地拟合了训练数据，但我们不认为这是一个预测房价的好模型。专业术语是，我们会说这个模型有**过拟合数据**，或者这个模型有过拟合问题。这个模型似乎不能推广到以前从未见过的新例子。

另一个术语是算法具有高方差。在机器学习中，许多人几乎可以互换地使用过度拟合和高方差这两个术语。过拟合或高方差背后的直觉是，算法非常努力地拟合每一个训练示例。结果是，如果你的训练集有一点点不同，比如说一个洞的定价多一点少一点，那么算法拟合的函数最终可能完全不同。

分类问题中也存在这样的问题：

![[Pasted image 20230207113932.png]]

𝑥 的次数越高，拟合的越好，但相应的预测的能力就可能变差。
问题是，如果我们发现了过拟合问题，应该如何处理？

## 4.2 解决过拟合（Addressing overfitting）

有几种方法：
1. 收集更多训练数据
2. 使用更少的特征
3. 

![[Pasted image 20230207114303.png]]

假设你拟合了一个模型，它有很高的方差，是过拟合。这是我们的过拟合房价预测模型。第一种解决方法是收集更多的训练数据，有了更大的训练集，学习算法就能学会拟合一个波动更小的函数。

![[Pasted image 20230207114550.png]]

解决过拟合的第二个选择是看看**是否可以使用更少的特征**。比如预测房价用到了很多特征，但如果没有足够的训练数据，那么你的学习算法也可能会过度拟合你的训练集。如果我们只选择最有用的功能的子集，你可能会发现你的模型不再严重过拟合。选择要使用的最合适的特征集有时也称为**特征选择**。

![[Pasted image 20230207115043.png]]

如果你看到一个过拟合模型，会发现参数通常比较大。现在如果你要消除这些特征中的一些，那就等于把这个参数设为0。事实证明，正则化是一种更温和地减少某些特性影响的方法，而不会像完全消除这些特性那样做得太苛刻。

**正则化所做的是鼓励学习算法缩小参数的值，而不一定要求参数设置为精确的0。**
        
补充：按照惯例，我们通常只是减小$w_j$参数的大小。

## 4.3 正则化的成本函数（Cost function with regularization）

如果用高阶的多项式，最终会得到一条超过拟合数据的曲线。如果我们能让这些高次项的系数接近于 0 的话，我们就能很好的拟合了。
因此，**我们不是最小化这个目标函数，而是线性回归的成本函数**。

![[Pasted image 20230207120059.png]]

修改后的函数如上图，我们只对w3和w4进行了正则化。但更普遍的是，如果你有很多特性，你可能不知道哪些是最重要的特性，哪些是要惩罚的。**正则化的实现方式通常是惩罚所有的特征**。

![[Pasted image 20230207121246.png]]

对于上图这个有100个特征的模型，我们对所有的参数都进行惩罚。公式可能变成上面这样。

这里的$\lambda$是正则化参数。

在这个修改的成本函数中，我们想要最小化原始成本，即均方误差成本加上正则化项。这个新的成本函数权衡了两个目标：
1. 尝试最小化第一项可以通过最小化预测值和实际值的平方差异来鼓励算法很好地拟合训练数据。
2. 试着最小化第二项，这将倾向于减少过拟合。

你选择的值，指定了相对重要性或相对权衡，或你如何在这两个目标之间取得平衡。

![[Pasted image 20230207121821.png]]

><small>我们来看看不同的值会导致你学习算法做什么。让我们用线性回归的房价预测例子。f(x)是线性回归模型。如果设为0，那么你根本不用正则化项，你就能拟合这条过度波动，过度复杂的曲线了。如果是一个非常大的数，那么你就在右边这个正则化项上施加了很大的权重。使它最小化的唯一方法就是确保所有w的值都非常接近0。如果非常非常大，学习算法会选择w1,w2,w3,w4这四个值都非常接近于0，因此f(x)基本上等于b，所以学习算法会拟合一条水平直线。</small>

## 4.4 正则化线性回归（Regularized linear regression）

![[Pasted image 20230207155431.png]]

正则化线性回归之后，梯度下降需要重复更新的公式有所改变，具体体现在求导的部分。由于我们没有对b进行正则化，因此b的导数没有改变。

## 4.5 正则逻辑回归（Regularized logistic regression）

正如逻辑回归的梯度更新看起来与线性回归的梯度更新惊人地相似，您会发现正则化逻辑回归的梯度下降更新看起来也与正则化线性回归的更新相似。

对于这个例子：

![[Pasted image 20230207160035.png]]

z是一个高阶多项式，它被传递到sigmoid函数中来计算f。你可能最终得到一个过于复杂和过拟合的决策边界作为训练集。

这是逻辑回归的成本函数，你需要添加正则化项。当你最小化这个成本函数作为w和b的函数时，它有惩罚参数$w_1, w_2,...,w_n$的效果，并防止它们太大。

我们还是用梯度下降法最小化这个成本函数吧。

![[Pasted image 20230207160313.png]]

加在这里。
