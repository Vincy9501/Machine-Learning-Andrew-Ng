# 1. 用逻辑回归分类（Classification with logistic regression）

## 1.1 动机（motivation）

上周我们学习了线性回归，它预测一个数字。本周，您将学习分类，其中输出变量y只能取少数几个可能值中的一个，而不是无限范围内的任何数字。线性回归并不是一个很好的分类算法。让我们来看看原因，这将引导我们进入另一种叫做逻辑回归的算法。

例子：
1. 判断一封电子邮件是否是垃圾邮件
2. 判断在线金融交易是否存在欺诈
3. 将肿瘤划分为恶性和非恶性

在这些问题中，你想要预测的变量只能是两个可能值中的一个。这种只有两种可能输出的分类问题称为二元分类。二进制这个词指的是只有两种可能的类或两种可能的范畴。在这些问题中，我将相对交替地使用**类**和**范畴**这两个术语。它们的意思基本上是一样的。

我们经常用no或yes来表示从句，有时也用false或true来表示，通常用数字0或1来表示。遵循计算机科学中的常见惯例，0表示假，1表示真。我通常会用数字0和1来表示答案y，因为这最适合我们想要实现的学习算法类型。

如何构建一个分类算法？

![[Pasted image 20230205134314.png]]

这是一个训练集的例子，用于分类肿瘤是否为恶性

现在，你可以在这个训练集上尝试的一件事是应用你已经知道的算法：线性回归，并试图拟合一条直线的数据。如果你这样做，直线可能是这样的：

![[Pasted image 20230205134359.png]]

所有的数字都在0到1之间或者小于0或者大于1。但这里我们想要预测类别，您可以尝试的一件事是选择一个阈值，例如0.5。因此，如果模型输出的值低于0.5，那么您就可以预测等于零或不是恶性的。如果模型输出的数字等于或大于0.5，则预测Y等于1或恶性。


请注意，阈值0.5在这一点与最佳拟合直线相交。如果你在这里画一条垂直线，左边的所有东西都预测y等于0。右边所有的结果都是y = 1。现在，对于这个特定的数据集线性回归似乎可以做一些合理的事情。但现在让我们看看如果你的数据集还有一个训练示例会发生什么。右边这边。我们再延伸一下横轴。注意，这个训练示例不应该真正改变数据点的分类方式。我们刚才画的这条垂直分界线仍然是有意义的，作为分界线，比这小的肿瘤应该归为零。大于此值的肿瘤应该归为1。

但是一旦你添加了极端的训练示例，比如当肿瘤很大时我们希望算法将其归类为恶性。但是我们会得到一条糟糕的线。

因此不推荐线性回归在分类问题中使用。

## 1.2 逻辑回归（Logistic regression）

让我们来谈谈逻辑回归，这可能是世界上使用最广泛的分类算法。让我们继续讨论肿瘤是否恶性的分类例子。我们将用标签1或者是阳性类别来代表恶性肿瘤，用0或者否和阴性例子来代表良性肿瘤。

![[Pasted image 20230205135054.png]]

这是一个数据集的图表，横轴是肿瘤大小，垂直轴只有0和1的值。线性回归并不是解决这个问题的好算法。相比之下，我们使用逻辑回归拟合一个像这样的曲线，s型曲线到这个数据集。

在这个例子中，如果一个病人x轴上标注的这么大的肿瘤，那么算法将输出0.7，这表明它更接近或更可能是恶性的。

为了建立逻辑回归算法，我想描述一个重要的数学函数，它被称为Sigmoid函数，有时也被称为逻辑函数。Sigmoid函数是这样的：

![[Pasted image 20230205135151.png]]

请注意，上下两x轴是不同的。上图x轴是肿瘤大小，所以都是正数。而在下图中，这里是0，横轴有正负两种值横轴标为z。

所以Sigmoid函数的输出值在0到1之间。函数表达式为：

$$g(z) = \frac1{1+e^{-z}} \qquad 0<g(z)<1$$

这就是为什么sigmoid函数有这样的形状它开始时非常接近于0然后慢慢地增加或增长到1。同样，在Sigmoid函数中，当z = 0时，g (z)等于0.5。

现在，让我们用这个来建立逻辑回归算法。我们分两步来做：

1. 线性回归函数可以定义$f_{\vec{w},b}(\vec x) = \vec w \cdot \vec x + b$，我们把这个值存储在一个变量中，称之为z。也就是说$z = \vec w \cdot \vec x + b$。
2. 取这个z的值并将它传递给Sigmoid函数，也叫逻辑函数g，现在g (z)输出一个由这个公式计算出来的值，它将在0到1之间。

当你把这两个方程放在一起，它们就给出了逻辑回归模型：
$$f_{\vec{w},b}(\vec x) = \frac 1 {1 + e^{-(\vec w \cdot \vec x + b)}}$$它所做的是输入一个特征或一组特征X然后输出一个0到1之间的数字。

我们回到肿瘤分类的例子。x是肿瘤大小，y是0或1。如果你有一个病人，她一定大小的肿瘤x输入这个模型获得的结果为0.7，这就意味着模型预测或者模型认为有70%的概率这个病人的y值等于1。换句话说，模型告诉我们，它认为病人的肿瘤有70%的几率是恶性的。

如果有一天你阅读研究论文或所有逻辑回归的博客，有时你会看到这个符号：
$$f_{\vec{w},b}(\vec x) =P(y=1|\vec x ;\vec w,b)$$这表示给定输入特征x、参数w和b，这里分号是用来表示w和b是影响计算的参数，在输入特征x的情况下y等于1的概率是多少。

## 1.3 决策边界（Decision boundary）

上一节，我们学习了逻辑回归模型。现在，让我们来看看决策边界，以更好地了解逻辑回归是如何计算这些预测的。

我们回想一下上一节中提到的，逻辑回归算法需要两步，第一步是让z=wx+b，第二步是把z带入sigmoid函数中。我们把结果解释为y = 1的概率，给定x，参数w和b，这可能是一个0.7或0.3的数字。

现在，如果你想学习预测的算法。y的值是0还是1 ?

你要做的一件事是设置一个阈值，超过这个阈值，你预测y是1，或者你设置y帽为预测值等于1，低于这个阈值，你可以说y帽，我的预测值等于0。一个常见的选择是选择阈值为0.5，这样如果f (x)大于或等于0.5，则预测y为1。我们把这个预测写成y帽等于1，或者如果f (x)小于0.5，那么预测y是0，换句话说，预测y帽等于0。

现在，让我们更深入地研究一下这个模型何时会预测到1。也就是g (z)什么时候大于等于0.5？这是一个Sigmoid函数，所以当z大于等于0时g (z)大于等于0.5。

那么z什么时候大于等于0 ? wx + b > 0 即可。

现在让我们来想象一下模型是如何进行预测的：

![[Pasted image 20230205142154.png]]

例如，这里有两个特征，x1和x2。红色的叉对应y = 1，蓝色的圆对应y = 0。逻辑回归模型将使用函数f (x) = g (z)进行预测。z现在是这个表达式：$$f_{\vec{w},b}(\vec x) = g(z) = g(w_1x_1+w_2x_2+b)$$在这个例子中，参数的值是w1 = 1，w2 = 1，b = - 3。现在让我们看看逻辑回归是如何进行预测的。我们来计算一下什么时候wx + b大于等于0什么时候wx + b小于0。要算出这个，有一条很有趣的直线，也就是当wx + b恰好等于0时，这条线也被称为**决策边界**。因为在这条线上，你对y = 0还是y = 1几乎保持中立。

对于我们的例子，这个决策边界就是$x_1$+$x_2$ - 3。$x_1$+$x_2$ - 3 = 0时的这条线就是决策边界。

现在让我们看一个更复杂的例子，其中决策边界不再是一条直线。和前面一样，叉表示类y = 1，小圆表示类y = 0。

![[Pasted image 20230205142501.png]]

z现在是这个表达式：$$f_{\vec{w},b}(\vec x) = g(z) = g(w_1x_1^2+w_2x_2^2+b)$$
通过选择这样的特征，多项式特征变成了逻辑回归。令：
$$z = w_1x_1^2+w_2x_2^2 - 1 = 0$$当$x_1²+ x_2²= 1$时，这个表达式等于0。

下一节，我们将从逻辑回归的成本函数开始，然后，弄清楚如何将梯度下降应用于它。

# 2. 逻辑回归的成本函数（Cost function for logistic regression）

## 2.1 逻辑回归的成本函数（Cost function for logistic regression）

请记住，成本函数为您提供了一种方法来衡量特定参数集与训练数据的拟合程度。从而给了你一种选择更好参数的方法。在这个视频中，我们将看看平方误差成本函数为什么不是逻辑回归的理想成本函数。我们将看看一个不同的成本函数，它可以帮助我们为逻辑回归选择更好的参数。

和前面一样，我们用m表示训练示例的数量。每个训练示例都有一个或多个特征，如肿瘤大小、患者年龄等，总共有n个特征。我们称这些特征为$x_1 ... x_n$。由于这是一个二进制分类任务，目标标签y只有两个值，要么是0，要么是1。最后，用该方程定义逻辑回归模型。

你要回答的问题是，给定这个训练集，如何选择参数w和b？


回忆一下线性回归，平方误差代价函数是$$J(\vec w, b) = \frac 1m \sum_{i=1}^m \frac 1 2(f_{(\vec w, b)}(\vec x^{(i)}-y^{(i)})^2$$成本函数看起来是一个凸函数或者碗形或锤形。梯度下降一步一步地收敛到全局最小值。

现在你可以用同样的成本函数来做逻辑回归。

如果$$f_{\vec{w},b}(\vec x) = \frac 1 {1 + e^{-(\vec w \cdot \vec x + b)}}$$然后用f (x)的值来画成本函数，就会像这样：

![[Pasted image 20230206110257.png]]

这意味着如果你尝试使用梯度下降法，有很多局部极小值你可以得到。事实证明，对于逻辑回归，这个平方误差成本函数不是一个好的选择。

梯度下降算法可以保证收敛到全局最小值。为了建立一个新的成本函数，我们将用它来进行逻辑回归。我要稍微改变一下成本函数J (w和b)的定义，特别是，如果你看这个和：
$$\frac 1 2(f_{(\vec w, b)}(\vec x^{(i)}-y^{(i)})^2$$
我们把这一项叫做单个训练例子中的损失。我要用大写的L来表示损失。

![[Pasted image 20230206111530.png]]

我们来看看为什么这个损失函数是有意义的。让我们先考虑y = 1的情况，然后画出这个函数来直观地了解这个损失函数是怎样的。损失函数衡量的是你在一个训练样本上做得有多好，它是通过把所有训练样本上的损失加起来得到的，成本函数衡量的是你在整个训练集上做得有多好。

![[Pasted image 20230206111610.png]]


f总是在0和1之间，因为逻辑回归的输出总是在0和1之间。函数中唯一相关的部分就是这部分，对应于f在0到1之间。让我们放大，仔细看看图表的这一部分。如果算法预测的概率接近1，而真实标签为1，则损失非常小。它几乎是0，因为你非常接近正确答案。现在继续以y = 1为例，如果算法的输出值是0.1如果它认为肿瘤是恶性的概率只有10%但y确实是1。

![[Pasted image 20230206112143.png]]

在这个图中，对应于y = 0，纵轴显示了不同f (x)值的损失值。当f为0或非常接近0时，损失也会非常小，这意味着如果真实标签为0，模型的预测非常接近0。

f (x)的值越大，损失就越大，因为预测值离真实值0越远。事实上，当预测接近1时，损失实际上接近无穷大。

回到肿瘤预测的例子，如果模型预测病人的肿瘤几乎肯定是恶性的，比如，99.9%的恶性概率，结果实际上不是恶性的，所以y = 0，那么我们对模型的损失非常大。

在y = 0的情况下，预测f (x)离y的真实值越远，损失越大。事实上，如果f (x)趋近于0，这里的损失就会变得很大，趋近于无穷。当真实标签为1时，算法强烈地激励不去预测太接近0的东西。

如果你能找到参数w和b的值，使这个最小化，那么你就有了一个很好的参数w和b的值集用于逻辑回归。

## 2.2 简化逻辑回归的成本函数（Simplified Cost Function for Logistic Regression）

这是我们在之前的视频中定义的逻辑回归的损失函数：

![[Pasted image 20230206112143.png]]

因为我们仍然在处理一个二元分类问题，y不是0就是1。因为y不是0就是1，不能取除0和1以外的任何值，我们可以用一种更简单的方法来写这个损失函数。

损失函数可以写成这样：

![[Pasted image 20230206112604.png]]

结果是这个方程，我们只用一行写出来，完全等价于上面这个更复杂的公式。使用这个简化的损失函数，让我们回过头来写出逻辑回归的成本函数。

![[Pasted image 20230206113009.png]]

这又是一个简化的损失函数。回想一下，成本J只是平均损失，整个训练集m个例子的平均损失。所以它有如下公式，如果你把上面的简化损失的定义代入，它就是1 / m乘以上面这一项的和。如果你把负号移到外面，那么你就得到了这个表达式，这是成本函数。

# 3. 梯度下降的实现（Gradient Descent Implementation）

为了拟合逻辑回归模型的参数，我们将尝试找到使成本函数J (w和b)最小的参数w和b的值，我们将再次应用梯度下降来做到这一点。
在本节我们将重点讨论如何找到参数w和b的一个好的选择。

![[Pasted image 20230206113448.png]]

这是通常的梯度下降算法，你反复更新每个参数。

把这些导数表达式代入这些项。这为逻辑回归提供了梯度下降。

![[Pasted image 20230206113614.png]]

现在，你可能会好奇一件有趣的事情是，这两个方程看起来很像我们之前得到的线性回归的平均值，所以你可能会想，线性回归实际上和逻辑回归是一样的吗?虽然这些方程看起来一样，但这不是线性回归的原因，是函数f (x)的定义变了。在线性回归中，f (x)等于，这是wx + b，但在逻辑回归中，f (x)被定义为应用于wx + b的sigmoid函数，虽然线性回归和逻辑回归的算法看起来是一样的，但实际上它们是两种非常不同的算法，因为f (x)的定义是不一样的。


